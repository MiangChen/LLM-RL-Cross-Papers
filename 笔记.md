RLingua: Improving Reinforcement Learning Sample Efficiency in Robotic Manipulations With Large Language Models

本文的使用RL和LLM共同训练一个机械臂, 用于完成物品的

本文的tricky在于以下几个点

- RL的框架是TD3, DDPG的优化版本, TD3是一种off policy RL算法, 框架是online RL framework的. 

- 使用了两个replay buffer, 分别存储LLM和RL的数据
- 在一次episode中, 在每个state下, 都有一部分概率选择LLM的策略来选择action, 另一部分概率选择training policy RL的action. 
- actor 的loss也是分为两个部分组成的, 一个是来自RL replay buffer的loss, 另一部分是来自LLM replay buffer的loss, 同时通过一个定值参数控制LLM部分的loss权重
- critic的loss只取决于RL replay buffer.

- 随着学习率的退火, 选择LLM的策略的概率也会退火, 这样可以逐渐克服LLM的控制策略在训练后期不佳的问题.

